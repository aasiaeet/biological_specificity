---
title: "Beta Mixture Rates Model for MicroRNA Cancer Type Specificity"
author: "Amir Asiaee"
date: '`r format(Sys.Date(), "%d %B, %Y")`' # Formats the date
output: 
  html_document:
    toc: true # Adds a table of contents
    toc_float: true # Floats the table of contents
    toc_depth: 2 # Sets depth of headers to include in TOC
    number_sections: true # Numbers the sections
    highlight: kate # Syntax highlighting style
    theme: yeti # Bootswatch theme
    fig_width: 10 # Width of figures in inches
    fig_height: 6 # Height of figures in inches
    css: ../styles.css # Link to an external CSS file
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction
The goal of this report is to figure out a score for tissue specificity of miRs. We use the TCGA dataset and therefore we use the "tissue" term loosely because in reality the labels are cancer type or tissue of origin. 

At the beginning, we do not have a concrete mathematical definition for tissue specificity. We only have some qualitative characteristics in mind that such a  definition should satisfy. 

First, tissue specificity of a miR should be independent of the rest or miRs. 

Also a miR is more tissue specific if it is expressed in smaller number of tissues. So any score for specificity should be a continuous value from 0 to infinity. 


This step is only for deciding which miR is informative for further analysis (such as classification of tissues, regression, etc.) so we do not need to worry about the effect of combination of miRs (redundency in specificity is not important, if miR1 and miR2 both are specific to breast, that is fine we keep both and let the down stream analysis decide what to do with the redundancy). 


## Model Summary
At most, we have two set of tissues (cancer type), one in which the miR is expressed and the rest in which it is not expressed. Note that miR1 can be expressed in ALL/NONE of the tissues. For that we assume that the rate of zeros for each tissue is generated by a mixture of two Beta distributions: one component for tissues expressing miR1 and the other one for the tissues that are not. Each rate comes from one component of the mixture and the two mixture can be different (then the miR is tissue specific because there is a divide between the rates) or very similar (then two mixture components are close to each other or one mixture proportion is close to zero). 

The goal of this report is to fit the new hierarchical model and generate a tissue specificity score. 

## Data Exploration
First we set up the paths to our data storage locations.
```{r paths}
rm(list = ls())
source("../00_paths.R")
```
Now we load the data set which consitst of TCGA samples that have matched mRNA and microRNAs with the only 5 glioblastoma samples removed.
```{r mir}
load(file.path(paths$clean, "matchedNoGBM.rda")) # about 800 MB
dim(miRdata)
rm(mRNAdata, colvec)
```

Here we put together "Table 1", a count of the number of samples from each cancer type.
```{r}
abbr <- read.csv(file.path(paths$clean, "cancerAbbrev.csv"), row.names = 1)
tab <- table(cancerType)
abbr <- abbr[names(tab), , drop=FALSE]
daft <- data.frame(abbr, tab)[, c(1,3)]
colnames(daft) <- c("Study Name", "Number of Samples")
write.csv(daft, file=file.path(paths$results, "sampleCount.csv"))
head(daft)
```
We (believe that we may) want to remove microRNAs that frequently give zero counts for downstream task.
So, we start by counting the number of such zeros.

```{r numberOfZeros, fig.cap="Histogram of the number of zero counts observed per miR.",fig.width=8,fig.height=6}
numberOfZeros <- apply(miRdata, 1, function(x) sum(x == 0))
print(paste("total number of microRNAs:", length(numberOfZeros)))
hist(numberOfZeros/ncol(miRdata), breaks=123, col="gray75", main="", 
     xlab="Fraction of zero counts per microRNA", ylab="Number of microRNAs")
```

Let's take a peak at extremes:
```{r neverSeen}
max(numberOfZeros) == ncol(miRdata)
n_always_absent <- sum(numberOfZeros == max(numberOfZeros))
print(paste("Number of microRNAs with zero count in all samples", n_always_absent))
names(which(numberOfZeros == max(numberOfZeros)))
n_always_present <- sum(numberOfZeros == 0)
print(paste("Number of microRNAs that are present in all samples", n_always_present))
```

We order that miRs according to the number of zeros they have, i.e., miRs with most zeros are processed first in our subsequent analysis:
```{r results}
nonzeros <- ncol(miRdata) - numberOfZeros
ordered_mir_data <- miRdata[order(nonzeros),]
nonzeros <- sort(nonzeros)
```

In our analysis, we aim to determine a cutoff for retaining miRs that are expressed in only a single or a select few cancer types, despite being undetected in others. One way to find that out would be to perform a chi-squared test comparing presence/absence with cancer cohort. The only problem with that idea when having nonzero elements is a rare event is that the normal approximation that justifies using the chi-squared test fails to hold.

# Bayesian Mixture Rates
So, we are going to use a hierarchical Bayesian model extending the one presented in Section 5.3 of the textbook by Gelman, Carlin, Stern, and Rubin (_Bayesian Data Analysis_, second edition). We are going to implement this idea here.

## The Hierarchical Model

The underlying assumption is that there are rate parameters $\Theta = \{\theta_t\}$ defined for each cancer type $t\in [T]$. 
These parameters are themselves assumed to arise from some common mixture of beta distribution $\theta_t \sim \sum_{k=1}^2 \pi_k \textrm{Beta}(\alpha_k, \beta_k)$, where $(\pi_1, \pi_2) = (\pi, 1 - \pi)$.  We can then determine the hyperparameters posterior distribution from the data, and we can use this to sample the $\Theta$'s from the posterior distribution. A couple of points to distinguish ourselves from Gelman's: 

## Hyperpriors 

We have five hyperparameters and we assume the following conditional independence between them: $P(\alpha_1, \beta_1, \alpha_2, \beta_2, \pi) = P(\alpha_1, \beta_1)P(\alpha_2, \beta_1)P(\pi)$. We assume $P(\pi)=1$, i.e., the mixture proportion's prior is uniform. For Beta parameter pairs we use Gelman's approach. A key point of the discussion in Gelman's book is to describe how to transform the hyperparameters $(\alpha, \beta)$ into a space where the prior can sensibly be taken to be uniform. The final conclusion is $P(\alpha, \beta) \propto \alpha \beta (\alpha + \beta)^{-5/2}$. The transformed space (over which we perform the parameter search) is $(x, y) = (\log(\frac{\alpha}{\beta}), \log(\alpha+\beta))$.

A key difference between our approach and Gelman's is that we are interested only in mixture of two Unimodal Beta distributions ($\alph , \beta > 1$), therefore $P(\alpha, \beta) \propto 0$ for parameters resulting in Bimodal Betas (U-shape distributions). Also, to handle the identifiability issue, we always assume that the second Beta distribution has a larger mean ($\frac{\alpha_2}{\alpha_2 + \beta_2} \ge \frac{\alpha_1}{\alpha_1 + \beta_1}$) and therefore set $P(\alpha_1, \beta_1, \alpha_2, \beta_2) = 0$ if this is not the case. 

In computing the hyperpriors, Gelman suggests to enlarge the grid over which we search so that we include contours that contain ~95% of the probability distribution. The computations is based on normal approximation and for us the contour values should be .0087 of the maximum value inside the grid (based on normal approximation of the distribution). Since this process is time consuming, we at most enlarge the contour twice which seems to be enough for our problem. 

## Fisher's Criterion == the Tissue Specificity Score

Having the posterior of the rates as a mixture of Beta distributions, the tissue specificity score is defined as follows: Sample from the posterior of the $\theta$ many times and keep track of which rate is from which of the two component (note that WLOG, by setting the prior properly, we can assume that component 1 is for tissues where the miR is absent and component 2 is for tissues where it is present) . Then compute the empirical (simulated) mean of samples in each components and also their variances. Then compute the Fisher's criterion (from linear discriminant analysis which shows how linearly separable two classes are) as $\frac{(\mu_2 - \mu_1)^2}{s_1^2 + s_2^2}$. If it is close to zero then the rates (i.e., the two clusters) are close to each other and therefore the miR is not tissue (cancer-type) specific. Note that the Fisher's Criterion can become arbitrarily large because the nominator is at most one but the denominator can gets very small. 


# Implementation

## Computing the Posterior of the Beta Components
Computing the posterior the Beta components (hyper-parameters) is the hardest part the method. We do that in this section and in the next section we sample from this posterior and then sample the rest of the parameters. 

First we need some code to compute the prior probabilities and likelihoods in Bayesian updates plus a numerical method for integrating the $\pi$ out. Below subsections provide that:

### Gamma Ratios
To save processing time, we compute the gamma ratios and store them. Given a $(x, y) = (\log(\frac{\alpha}{\beta}), \log(\alpha+\beta))$ point and two vectors of number or successes (K) and number of trials (N) per tissue, the below code computes the ratio $\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} \frac{\Gamma(\alpha+y[t])\Gamma(\beta+n[t]-y[t])}{\Gamma(\alpha+\beta+n[t])}$ for all $T$ tissues and returns a row of those values. We keep track of corresponding $\alpha$ and $\beta$ parameters in the `params` matrix and their gamma ratios in the `vals` matrix. We also save the grid coordinates along with the index of those parameters in the given parameter set. Note that we support both grid and pairs of parameters as inputs. Finally, to avoid numerical issues we compute the log of the ratios. 


```{r gamma ratio computation, warning = FALSE}
source("../methods/01_gamma_ratios.R")
```

### HyperPriors
The two parameter sets $(\alpha_1, \beta_1)$ and $(\alpha_2, \beta_2)$ are assumed to be independent and with $\propto (\alpha+\beta)^{-5/2}$ prior. Translating this to the grid parameter space we get an extra term from the Jacobian $P(\log(\frac{\alpha}{\beta}), \log(\alpha + \beta)) \propto \alpha \beta (\alpha+\beta)^{-5/2}$ as the prior. For each point on the grid of parameters $(\alpha_1, \beta_1, \alpha_2, \beta_2)$ we compute the log hyperprior below. Note that we set the prior to zero (or log of it it -Inf [or the smallest possible number: `.Machine$longdouble.min.exp`]) when the mean1 > mean2 and when either of the Beta distributions become bimodal.
```{r hyperpriors, warning = FALSE}
source("../methods/02_hyper_priors.R")
```


### Integral with Respect to Pi
Because we have a mixture model, to compute the posterior of the Beta model hyperparameters we need to marginalize out (integrate out) the other parameter $\pi$ where $(\pi, 1-\pi)$ is the mixture proportion vector. To do this numerically, we consider discrete points with dPi $=\Delta\pi=1/100$ distance from each other and use the below function to compute logarithm of $\int_0^1 P(\pi) \prod_t \Big( \pi \frac{\Gamma(\alpha_1+\beta_1)}{\Gamma(\alpha_1)\Gamma(\beta_1)}  \frac{\Gamma(\alpha_1+y[t])\Gamma(\beta_1+n[t]-y[t])}{\Gamma(\alpha_1+\beta_1+n[t])} +  (1-\pi) \frac{\Gamma(\alpha_2+\beta_2)}{\Gamma(\alpha_2)\Gamma(\beta_2)}  \frac{\Gamma(\alpha_2+y[t])\Gamma(\beta_2+n[t]-y[t])}{\Gamma(\alpha_2+\beta_2+n[t])} \Big)$.
Note that taking log and subtracting maximum and then exponentiate is the standard approach to avoid numerical instability. 
```{r integrate pi out, warning = FALSE}
source("../methods/03_integral_over_pi.R")
```

### Finding a Sufficiently Large Grid for Posterior of Hyperparameters
Now we are ready for sampling from the posterior of the hierarchical model. To do so, we first need to sample from the posterior of the hyperparameters; the rest is easier. So in the constructor of the BetaMixtureRates object below, we make the approximation of of the transformed Beta posteriors as step functions over a sufficiently large grid and save it in the object. The rest of the computation will be done whenever the `sampleFromPosterior` is called.

The idea is that any d-dimensional unimodal distribution which is roughly symmetric can be approximated with a normal distribution with diagonal covariance. Then the log of that distribution can get approximated with a chi-squared distribution with d-degree of freedom for which we can get the parameter that gives us 95% of the probability mass. In our case d = 4 and we should find the pick of the empirical posterior distribution, C, and then compute C exp(-1/2 * 9.49) = C * 0.008695062 $\approx$ C * 0.0087 as the minimum value of the posterior that we want to capture in the grid. This means that if the computed posterior at any point on any of the faces of the hypergrid is greater than maxOverGrid * 0.0087 then we need to expand the hypergrid in that face direction. 

Each n dimensional cube has 2n faces. A face is where one of the dimensions is fixed to its min or max and the rest can vary. So in our case we find the minimum and maximum for all of the four coordinates and then fix one of them to its min or max and look at all the posterior values on the face. Example: (min, 1:100, 1:100, 1:100) or (1:100, 1:100, max, 1:100). Then we take the max of these elements, e.g., max(min, 1:100, 1:100, 1:100), and if it is greater than maxOverGrid * 0.0087 then we expand the face by subtracting/adding something to the min/max value. We do this in the repeat loop below. To save computation time, we do this at most twice which seems to be sufficient for most of our use cases. 
```{r BetaMixtureRates, warning = FALSE}
source("../methods/04_beta_mixture_rate_class.R")
```


## Sampling from the Joint Posterior 
Next we sample from the joint posterior of the parameters give the data. We use the chain rule to first sample the hyperpriors given the data and then the other paramters given the sampled hyperpriors. After sampling from the posterior, we compute the Fisher Criterion (or as named in the paper: Presence-Absence Contextuality (PAC) score) and save it in the object. 
```{r , warning = FALSE}
source("../methods/05_posterior_sampling.R")
```


# TCGA's microRNA Analysis
Now, we run our Bayesian inference pipeline on the microRNA data from the TCGA. We have already sorted them based on the number of non-zero elements. So we skip microRNAs that are always zero (of course they should get discarded) or always one (they should get retained, but if they are not variable they may not get selected in the selection process for the downstream analysis) for this analysis. 
```{r giant}
library(dplyr)
set.seed(12345)
beansdir<- file.path(paths$scratch, "BeanMixture")
if (!file.exists(beansdir)) dir.create(beansdir)
res <- 300

f <- file.path(paths$scratch, 'allStat.rda')
g <- file.path(paths$scratch, 'brlist.rda')
h <- file.path(paths$scratch, 'postlist.rda')

if (file.exists(f)) {
  load(f)
  load(g)
  load(h)
} else {
  postlist <- brlist <- list()
  allStat <- c()
  for (I in 1:length(nonzeros)) { #for each miR
    if (nonzeros[I] < n_always_absent) next #skip the first 4 miRs, they are always zero
    if (nonzeros[I] > n_always_present) break #skip the last 134 miRs, they are always present.
    print(I)
    mir <- rownames(ordered_mir_data)[I]
    # make a binary vector of zero or not.
    X <- c("NonZero", "Zero")[1 + 1*(ordered_mir_data[I,] == 0)]
    cat("Working on", mir, "\n", file = stderr())
    tab <- table(cancerType, X)
    K <- tab[,1]
    N <- apply(tab, 1, sum)
    # Compute the posterior of the hyperpriors
    br <- BetaMixtureRates(K, N, dPi=1/10, massTrsh=0.87, resolution=10)
    brlist[[mir]] <- br
    # Sample from the joint posterior distribution.
    post <- samplePosteriorRates(br, dPi=1/10, nsamp = 4000)
    postlist[[mir]] <- post
    
    mt <- apply(post$theta, 2, mean)
    O <- order(mt)
    png(file=file.path(beansdir, paste(mir, "png", sep='.')), 
        width=16*res, height=9*res, res=res, bg="white")
    boxplot(as.matrix(post$theta[, O]), col=cohortColors[O], cex.axis=0.7,
            ylab="Probability of Being NonZero",
            main=paste("Posterior Dist. of Thetas for", mir, "Tissue Specificity = ", 
                       formatC(post$fisherC, format = "e", digits = 2)), 
            ylim=c(0,1), las=2)
    dev.off()
  
    ourstats <- c(mir, nonzeros[I], mt, post$fisherC)
    allStat <- rbind(allStat, ourstats)
      
  }
  rownames(allStat) <- allStat[,1] #mirs as rownames
  allStat <- allStat[,-1]
  colnames(allStat)[c(1, 33)] <- c("nonzero", "fisherC")
  allStat <- matrix(as.numeric(allStat), ncol = ncol(allStat), dimnames = dimnames(allStat))

  save(allStat, file = f)
  save(brlist, file = g)
  save(postlist, file = h)
}
rm(f, g, h)
```


## Inspecting the Specificity Statistics
Let's take a look at specificity (Fisher) scores distributions for all miRs:
```{r , warning = FALSE}
dim(allStat)
fc <- allStat[,33]
sum(fc > 1)
sum(fc > 100)
fc[order(fc,decreasing = T)[1:4]]
hist(fc[fc<100], breaks = 100, main = "Biological Specificity Scores")
```
There are four miRs which are extremely specific (the BSS is great than 300). After that the score drops to 21. 
There are 199 miRs with specificity > 1. 
Let's plot the extremes:

```{r manip, warning = FALSE}
for(mir in rownames(allStat)[which(fc > 50)]){
  x <- allStat[mir, 2:32]
  par(mfrow=c(1,2))  
  hist(x, breaks = 50, main = paste("Tissue Separation by miR", mir));
  O <- order(x)
  boxplot(as.matrix(postlist[[mir]]$theta[, O]), col=cohortColors[O], cex.axis=0.7,
        ylab="Probability of Being NonZero",
        main=paste("Posterior Dist. of Thetas for", mir, "Tissue Specificity = ", 
                   formatC(postlist[[mir]]$fisherC, format = "e", digits = 2)), 
        ylim=c(0,1), las=2)
}
```

Interestingly, in all these four cases the miRNA separates OV, COAD, READ from the rest of cancer types, while the expression pattern is different: in the first one it is only present in those cancer (a clear case of why we should keep `hsa-miR-3913-5p`) while in the next three it is present everywhere except in those three. 

Let's skip the extremes and take another look at the histogram and the Presence-Absence Contextuality (PAC) scores' tail:
```{r truncHist, warning = FALSE}
hist(fc[-which(fc > 50)], breaks = 50, main = "PAC Score")

for(mir in rownames(allStat)[which(fc > 10 & fc < 50)]){
  x <- allStat[mir, 2:32]
  par(mfrow=c(1,2))  
  hist(x, breaks = 50, main = paste("Tissue Separation by miR", mir));
  O <- order(x)
  boxplot(as.matrix(postlist[[mir]]$theta[, O]), col=cohortColors[O], cex.axis=0.7,
        ylab="Probability of Being NonZero",
        main=paste("Posterior Dist. of Thetas for", mir, "Tissue Specificity = ", 
                   formatC(postlist[[mir]]$fisherC, format = "e", digits = 2)), 
        ylim=c(0,1), las=2)
}
```
Still it seems that large FC corresponds to clear separability of means, i.e., we have two clusters of tissues. 

## MicroRNAs with High PAC Score are Different from Highly Variable MicroRNAs

To demonstrate that PAC score captures biological significance, we compare the efficacy of microRNAs (miRs) selected based on high PAC scores against miRs chosen for their high variability. This comparison aims to elucidate whether PAC scores can indeed reflect biological relevance similar to or better than traditional methods of feature selection based on variability.


First, let's remember that the data was CPM normalized:
```{r , warning = FALSE}
hist(miRdata[1,], breaks = 123)
```

 
An essential step in the transcriptome analysis pipeline involves feature selection, especially for data characterized by frequent zeros or generally constant features. Eliminating such features is critical for enhancing the performance of downstream tasks. The conventional approach to determining high variability involves calculating the mean and variance of log-transformed expression levels and selecting miRs that stand out in terms of variability. We do this by fitting a Lowess curve to the variance vs. mean of log transformed expressions and select the miRs whose variance is greater than the variance predicted by this curve. This process highlights miRs that exhibit significant expression changes across samples, potentially reflecting crucial biological processes or states.

```{r , warning = FALSE}
# Log-transform the data (adding 1 to avoid log(0))
log_miRdata <- log2(miRdata + 1)

mean_log_expr <- rowMeans(log_miRdata)
var_log_expr <- apply(log_miRdata, 1, var)

# Fit a lowess curve to the variance vs. mean log expression
lowess_fit <- lowess(mean_log_expr, var_log_expr)

# Plot the variance vs. mean log expression with the lowess curve
plot(mean_log_expr, var_log_expr, xlab = "Mean of Log Expression", ylab = "Variance of Log Expression",
     main = "Variability of microRNAs with Lowess Fit", pch = 20, col = rgb(0.2, 0.4, 0.6, 0.4))
lines(lowess_fit, col = "red", lwd = 2)

# Identify microRNAs with variance above the lowess fit as highly variable
high_var_indices <- var_log_expr > approx(lowess_fit$x, lowess_fit$y, xout = mean_log_expr)$y
print(paste("Total number of highly variable microRNAs", sum(high_var_indices)))
number_of_highly_variable_mirs <- sum(high_var_indices)

high_var_miRs <- rownames(miRdata)[high_var_indices]
top_high_var_miRs <- head(high_var_miRs[order(-var_log_expr[high_var_indices])], number_of_highly_variable_mirs)
```


We select the similar number of miRs from the list of miRs sorted decreasingly according to their PAC scores. The goal is to determine if the two lists are similar. 
```{r , warning = FALSE}
sorted_mir_pac_score <- names(fc)[order(fc, decreasing = T)] 
top_high_spec_miRs <- head(sorted_mir_pac_score, number_of_highly_variable_mirs)
```

Let's check if the two sets share a lot of elements:
```{r , warning = FALSE}
# Assuming top_high_var_miRs and top_high_spec_miRs are your microRNA sets

# Initialize a vector to store Jaccard distances
jaccard_distances <- numeric(number_of_highly_variable_mirs)

# Calculate Jaccard distances for set sizes 1 to 100
for (i in 1:number_of_highly_variable_mirs) {
  set1 <- top_high_var_miRs[1:i]
  set2 <- top_high_spec_miRs[1:i]
  
  # Calculate intersection and union
  intersection_size <- length(intersect(set1, set2))
  union_size <- length(union(set1, set2))
  
  # Calculate Jaccard distance
  jaccard_distances[i] <- 1- (intersection_size / union_size)
}

# Plot the Jaccard distances
plot(jaccard_distances, type = "b", pch = 19, xlab = "Set Size", ylab = "Jaccard Distance",
     main = "Jaccard Distance between Top microRNAs Sets", xlim = c(1, number_of_highly_variable_mirs))

plot(jaccard_distances[1:100], type = "b", pch = 19, xlab = "Set Size", ylab = "Jaccard Distance",
     main = "Jaccard Distance between Top microRNAs Sets", xlim = c(1, 100))

```
In the first 100 microRNA the two sets share only 12 microRNAs, i.e., the set of highly variable genes and highly specific genes are approximately mutually exclusive. In other words, if we focus on highly variable genes we will miss the highly specific genes. Even if we increase the set size up to 568 (total number of highly variable microRNAs), the Jaccard distance is still .5. 

## MicroRNAs with High PAC Score are Informative
Here, we juxtapose miRs selected based on their high variability with those chosen for their high PAC scores. The aim is to assess the informational content of miRs selected via each criterion, particularly focusing on the biological insights they provide. We do this by using selected miRs by two criteria to perform cancer type classification. We run SVM 10 times while incrementally increasing the number of microRNAs considered by 10 for each of the two sets of selected miRNAs, and then plotting the performance of these selection methods on cancer type classification. 

### Intelligent Highly Specific Feature Addition

```{r , warning = FALSE}
allStat_filtered <- allStat[, -c(1, ncol(allStat))]
miR_mean_representation <- allStat_filtered[rownames(allStat_filtered) %in% top_high_spec_miRs, ]
correlation_matrix <- cor(t(miR_mean_representation))


# Initialize the list of selected miRs
selected_miRs <- c(top_high_spec_miRs[1])  # Start with the first miR

# Iterate through the miRs in 'top_high_spec_miRs'
for (miR in top_high_spec_miRs[-1]) {  # Skip the first since it's already added
    # Assume the miR can be added unless we find a correlation >= 0.5
    can_add_miR <- TRUE
    
    # Check correlation with each already selected miR
    for (added_miR in selected_miRs) {
        cor_with_added_miR <- correlation_matrix[miR, added_miR]
        
        # If correlation with any added miR is 0.5 or more, break and do not add this miR
        if (cor_with_added_miR >= 0.5) {
            can_add_miR <- FALSE
            break
        }
    }
    
    # Add miR to the selected list if all correlations were less than 0.5
    if (can_add_miR) {
        selected_miRs <- c(selected_miRs, miR)
    }
}

# Print the selected miRs
print(selected_miRs)
```

### Train and Test
```{r , warning = FALSE}
library(xgboost)
library(e1071)
library(caret)

set.seed(1234)

fit_my_model <- function(model_name, x, y) {
  if (model_name == "svm") {
    return(svm(x, y))
  } else if (model_name == "logistic") {
    return(multinom(x, y))  # Using nnet package for multinomial logistic regression
  } else if (model_name == "random_forest") {
    return(randomForest(x, y, type = "classification"))
  } else if (model_name == "xgboost") {
    data_dmatrix <- xgb.DMatrix(data = as.matrix(x), label = as.numeric(y) - 1)  # XGBoost requires zero-based indexing for labels
    params <- list(objective = "multi:softmax", num_class = length(unique(y)))
    return(xgb.train(params, data_dmatrix, nrounds = 100))  # nrounds can be adjusted
  } else if (model_name == "naive_bayes") {
    return(naiveBayes(x, y))
  } else {
    stop("Unsupported model type")
  }
}

my_predict <- function(model_name, model, new_x){
  prediction <- predict(model, new_x)
  if(model_name == "xgboost"){
    prediction <- factor(prediction + 1, levels = 1:length(levels(Y_test)))
    levels(prediction) <- levels(Y_test)  
  }
  return(prediction)
} 




f <- file.path(paths$scratch, "svm_results.rda")
rounds <- 5 #how many rounds you incrementally add features
step_length <- 10 #number of features you add each round
n_folds <- 5
set.seed(12345)

if(file.exists(f)){
  load(f)
} else {
  # Prepare data
  X <- as.data.frame(t(miRdata))
  X$ctype <- as.factor(cancerType)
  n_cancers <- length(unique(X$ctype))
  
  # Parameters
  folds <- sample(rep(1:n_folds, length.out = nrow(X)))
  accuracies <- list()
  all_models <- list()
  conf_matrices <- list()
  models_names <- c("svm")
  
  # Initialize confusion matrices and accuracies for each feature selection method and model
  for (feature_selection_method in c("High_Var", "High_Spec", "Diverse_High_Spec", "Combined")) {
    for (model_name in models_names) {
      storage_index <- paste0(feature_selection_method, "_", model_name)
      for (i in seq_len(rounds)) {
        conf_matrices[[storage_index]][[i]] <- matrix(0, ncol = n_cancers, nrow = n_cancers)
      }
      accuracies[[storage_index]] <- matrix(0, nrow = rounds, ncol = n_folds)
    }
  }
  
  for (i in seq_len(rounds)) {
    num_features <- i * step_length
    
    for (fold in seq_len(n_folds)) {
      test_indices <- (folds == fold)
      X_train <- X[!test_indices, ]
      X_test <- X[test_indices, ]
      
      features_list <- list(
        High_Var = top_high_var_miRs[1:num_features],
        High_Spec = top_high_spec_miRs[1:num_features],
        Diverse_High_Spec = selected_miRs[1:num_features],
        Combined = union(top_high_var_miRs[1:(num_features/2)], top_high_spec_miRs[1:(num_features/2)])
      )
      
      for (feature_selection_method in names(features_list)) {
        features <- features_list[[feature_selection_method]]
        
        X_train_subset <- X_train[, features, drop = FALSE]
        Y_train <- X_train[, "ctype"]
        X_test_subset <- X_test[, features, drop = FALSE]
        Y_test <- X_test[, "ctype"]
        
        for (model_name in models_names){
          # as.data.frame is for xgboost
          model <- fit_my_model(model_name, x = as.matrix(X_train_subset), y = Y_train)
          prediction <- my_predict(model_name, model, as.matrix(X_test_subset))
          cm <- confusionMatrix(prediction, Y_test)
          storage_index <- paste0(feature_selection_method, "_", model_name)
          accuracies[[storage_index]][i, fold] <- cm$overall['Accuracy']
          conf_matrices[[storage_index]][[i]] <- conf_matrices[[storage_index]][[i]] + as.matrix(cm$table)
        }
      }
      print(paste("Iteration", i, "fold", fold, "completed."))
    }
  }
  # Normalize confusion matrices after all folds
  for (feature_selection_method in c("High_Var", "High_Spec", "Diverse_High_Spec", "Combined")) {
    for (model_name in models_names) {
      for (i in seq_len(rounds)) {
        storage_index <- paste0(feature_selection_method, "_", model_name)
        conf_matrices[[storage_index]][[i]] <- conf_matrices[[storage_index]][[i]] / n_folds
      }
    }
  }
  save(accuracies, conf_matrices, models_names, features_list, rounds, n_folds, file = f)
}
```

### Transforming the list into a data frame suitable for plotting with ggplot2

```{r , warning = FALSE}
# Transforming the list into a data frame suitable for plotting with ggplot2
accuracy_data <- data.frame()
for (feature_selection_method in c("High_Var", "High_Spec", "Diverse_High_Spec", "Combined")) {
  for (model_name in models_names) {
    storage_index <- paste0(feature_selection_method, "_", model_name)
    
    for (round in seq_len(rounds)) {
      for (fold in seq_len(n_folds)) {
        accuracy_data <- rbind(accuracy_data, data.frame(
          FeatureSelection = feature_selection_method,
          Model = model_name,
          NumberOfFeatures = round * step_length,
          Fold = fold,
          Accuracy = accuracies[[storage_index]][round, fold]
        ))
      }
    }
  }
}
```

### Plotting
```{r , warning = FALSE}
library(dplyr)

accuracy_summary <- accuracy_data %>%
  group_by(FeatureSelection, NumberOfFeatures) %>%
  summarise(
    MeanAccuracy = mean(Accuracy),
    SE = sd(Accuracy) / sqrt(n()),   # Standard Error
    .groups = 'drop'
  ) %>%
  mutate(
    LowerCI = MeanAccuracy - SE * 1.96,  # Assuming a 95% confidence interval
    UpperCI = MeanAccuracy + SE * 1.96
  )

# Update FeatureSelection labels for better readability
accuracy_summary$FeatureSelection <- factor(accuracy_summary$FeatureSelection,
  levels = c("High_Var", "High_Spec", "Diverse_High_Spec", "Combined"),
  labels = c("Highly Variable Genes", "High Specificity Genes", "Diverse High Specificity Genes", "Combined Method")
)

library(ggplot2)

# Define custom colors for each method
method_colors <- c("Highly Variable Genes" = "red", 
                   "High Specificity Genes" = "blue", 
                   "Diverse High Specificity Genes" = "purple",
                   "Combined Method" = "green")

# Plotting
accuracy_plot <- ggplot(accuracy_summary, aes(x = NumberOfFeatures, y = MeanAccuracy, color = FeatureSelection, group = FeatureSelection)) +
  geom_line() +
  geom_point(size = 3, shape = 19) +  # Adding points with specified size and shape
  geom_ribbon(aes(ymin = LowerCI, ymax = UpperCI, fill = FeatureSelection), alpha = 0.2) +  # Adding shaded confidence interval
  scale_color_manual(values = method_colors) +
  scale_fill_manual(values = method_colors) +  # Ensure fill colors match line colors
  labs(title = "Model Accuracy by Feature Selection Method",
       x = "Number of Features",
       y = "Average Test Accuracy",
       color = "Feature Selection Method",
       fill = "Feature Selection Method") +  # Label for the legend
  theme_minimal() + 
  theme(legend.position = c(0.9, 0.1),  # Sets the position of the legend inside the plot
        legend.justification = c(1, 0),  # Anchors the legend at the bottom right
        legend.background = element_rect(fill = "white", colour = "black"),  # Optional: Adds a background to the legend for visibility
        legend.box.background = element_rect(color = "black", size = 0.5))  # Optional: Adds border to the legend box

print(accuracy_plot)

```


## Viszualizing Error Sources
Plotting the accuracies:

```{r , warning = FALSE}

library(ggplot2)
library(reshape2)

# Compare average confusion matrices for a specific feature set size
i <- 1  # Example: Compare for 100 features (10th iteration)
hvg_index <- paste0("High_Var", "_", "svm")
hsg_index <- paste0("Diverse_High_Spec", "_", "svm")

conf_diff <- conf_matrices[[hvg_index]][[i]]  - conf_matrices[[hsg_index]][[i]]
diag(conf_diff) <- 0
col_sums <- colSums(conf_diff)

hvg_per_class_mistake <- conf_matrices[[hvg_index]][[i]]
diag(hvg_per_class_mistake) <- 0
col_sums_hvg <- colSums(hvg_per_class_mistake)

hsg_per_class_mistake <- conf_matrices[[hsg_index]][[i]]
diag(hsg_per_class_mistake) <- 0
col_sums_hsg <- colSums(hsg_per_class_mistake)

bar_to_plot <- c()
for(i in 1:length(col_sums)){
  if(col_sums[i] >= 0){
    bar_to_plot[i] <- col_sums[i] / col_sums_hvg[i]
  } else {
    bar_to_plot[i] <- col_sums[i] / col_sums_hsg[i]
  }
}
bar_to_plot <- bar_to_plot * 100;
names(bar_to_plot) <- colnames(hsg_per_class_mistake)
# Create a data frame for plotting column sums
col_sums_df <- data.frame(Var2 = names(bar_to_plot), Sum = bar_to_plot)

# Normalize these sums for plotting (if necessary)
col_sums_df$Sum <- col_sums_df$Sum / max(abs(col_sums_df$Sum))  # Normalize to max sum for proportional sizing

# Determine the color scale dynamically based on values
col_sums_df$color <- ifelse(col_sums_df$Sum >= 0, "red", "blue")

library(ggplot2)
library(reshape2)

# Start ggplot
p <- ggplot(data = conf_diff_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +  # Heatmap
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
  geom_col(data = col_sums_df, aes(x = Var2, y = Sum, fill = Sum), width = 1, show.legend = FALSE) +
  scale_fill_gradient(name = "Bar Scale", low = "blue", high = "red", limits = c(min(col_sums_df$Sum), max(col_sums_df$Sum))) +
  coord_fixed(ratio = 1) +  # Ensure the tiles are square by fixing aspect ratio
  labs(x = "Actual Class", y = "Predicted Class", fill = "Difference in Classifications") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

print(p)


# Melt the difference matrix for ggplot
conf_diff_long <- melt(conf_diff, varnames = c("Var1", "Var2"))
names(conf_diff_long) <- c("Var1", "Var2", "value")

library(ggplot2)
library(reshape2)

# Assuming all data preparation steps as defined earlier

# Adjust the width and position of the bars
bar_width <- 0.5  # Width of the bars; adjust as needed to fit the plot aesthetics

# Start ggplot
p <- ggplot(data = conf_diff_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +  # Heatmap
  scale_fill_gradient2(low = "blue", mid = "white", high = "red", midpoint = 0) +
  coord_fixed(ratio = 1) +  # Ensure the tiles are square by fixing aspect ratio
  labs(x = "Actual Class", y = "Predicted Class", fill = "Normalized Difference") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

print(p)



```





### Variable Filtering

Here we want to pick top x% of miRs ordered decreasingly in PAC score. 

```{r varFiltering, warning = FALSE}
ssMirs <- names(fc)[order(fc, decreasing = T)] 
cutOffs <- floor(quantile(1:length(ssMirs), probs=c(seq(0.02, 0.1, .02), seq(0.15, 0.3, .05), seq(.4, 1, .1))))

featureList <- list()
for(I in 1:length(cutOffs)){
  featureList[[I]] <- ssMirs[1:cutOffs[I]]
}


```


# Appendix
It's always a good idea to finish up by reporting which versions of the
software tools were used for this analysis:
```{r si}
sessionInfo()
```
